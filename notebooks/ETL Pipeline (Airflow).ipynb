{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36445644",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook will connect to Redshift (IaC) using Spark and JDBC driver and put everything together by building an ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4a1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.dummy import DummyOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a438a8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get config object from config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec602a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve access key and secret key\n",
    "aws_access_key_id = config['AWS']['aws_access_key_id']\n",
    "aws_secret_access_key = config['AWS']['aws_secret_access_key']\n",
    "\n",
    "role_arn = config['Redshift']['role_arn']\n",
    "region_name = config['Redshift']['region']\n",
    "bucket_name = config['S3']['bucket_name']\n",
    "region_name = config['Redshift']['region']\n",
    "\n",
    "user_name = config['Redshift']['user_name']\n",
    "password = config['Redshift']['password']\n",
    "host = config['Redshift']['host']\n",
    "port = config['Redshift']['port']\n",
    "schema = config['Redshift']['schema']\n",
    "database = config['Redshift']['database']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711e842",
   "metadata": {},
   "source": [
    "## Put it all together - Build an ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b0bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add default arguments\n",
    "default_args = {\n",
    "    'owner': 'udacity',\n",
    "    'start_date': datetime(2019, 1, 12),\n",
    "    'depends_on_past': False,\n",
    "    'email_on_retry': False,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'catchup': False,\n",
    "    'on_failure_callback': logging.error(\"Failed to execute 'udac_example_dag' DAG\")\n",
    "}\n",
    "\n",
    "# create the main DAG\n",
    "dag = DAG('nyse-stock',\n",
    "          default_args=default_args,\n",
    "          description='an end-to-end ETL Pipeline from feching API to S3 to Redshift',\n",
    "          schedule_interval='0 * * * *'\n",
    "          )\n",
    "\n",
    "# create a dummy operator\n",
    "start_operator = DummyOperator(task_id='Begin_execution',  dag=dag)\n",
    "\n",
    "# fetch stock data from RapidAPI\n",
    "fetch_stock_data_task = FetchAPIOperator(\n",
    "    task_id=\"fetch_stock_data_task\",\n",
    "    dag=dag,\n",
    "    config = config,\n",
    "    symbol = config['Vars']['symbol'],\n",
    "    rangeData = config['Vars']['rangeData'],\n",
    "    interval = config['Vars']['interval']\n",
    ")\n",
    "\n",
    "# process indicators table\n",
    "process_indicators_table_task = ParseIndicatorsOperator(\n",
    "    task_id=\"process_indicators_table_task\",\n",
    "    dag=dag,\n",
    "    symbol = config['Vars']['symbol'],\n",
    "    rangeData = config['Vars']['rangeData'],\n",
    "    interval = config['Vars']['interval']\n",
    ")\n",
    "\n",
    "# process metadata table\n",
    "process_metadata_table_task = ParseMetaDataOperator(\n",
    "    task_id=\"process_metadata_table_task\",\n",
    "    dag=dag,\n",
    "    symbol = config['Vars']['symbol'],\n",
    "    rangeData = config['Vars']['rangeData'],\n",
    "    interval = config['Vars']['interval']\n",
    ")\n",
    "\n",
    "# upload metadata table to S3\n",
    "upload_to_S3_task = UploadS3Operator(\n",
    "    task_id=\"upload_metadata_S3_task\",\n",
    "    dag=dag,\n",
    "    config =  config,\n",
    "    symbol = config['Vars']['symbol'],\n",
    "    rangeData = config['Vars']['rangeData'],\n",
    "    interval = config['Vars']['interval']\n",
    ")\n",
    "\n",
    "# migrate S3 to redshift\n",
    "migrate_S3_to_redshift_task = MigrateS3RedshiftOperator(\n",
    "    task_id=\"migrate_S3_to_redshift_task\",\n",
    "    dag=dag,\n",
    "    config =  config,\n",
    "    symbol = config['Vars']['symbol'],\n",
    "    rangeData = config['Vars']['rangeData'],\n",
    "    interval = config['Vars']['interval']\n",
    ")\n",
    "\n",
    "# include data quality\n",
    "data_quality_check_task = CheckDataQualityOperator(\n",
    "    task_id=\"data_quality_check_task\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# create a dummy operator\n",
    "end_operator = DummyOperator(task_id='Stop_execution',  dag=dag)\n",
    "\n",
    "# Add the dependencies\n",
    "start_operator >> fetch_stock_data_task\n",
    "\n",
    "fetch_stock_data_task >> [process_indicators_table_task, process_metadata_table_task]\n",
    "\n",
    "[process_indicators_table_task, process_metadata_table_task] >> upload_to_S3_task\n",
    "\n",
    "upload_to_S3_task >> migrate_S3_to_redshift_task\n",
    "\n",
    "migrate_S3_to_redshift_task >> data_quality_check_task\n",
    "\n",
    "data_quality_check_task >> end_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dd0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
